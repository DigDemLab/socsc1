language=pbsapply(txt.clean, function(x){textcat(paste0(x$txt, collapse = ' '))}),
txt.merged=sapply(txt.clean, function(x){paste0(x$txt, collapse = '\n')}),
txt.merged.utf8=iconv(txt.merged, from = "UTF-8", to = "MAC") %>% iconv(., from = "MAC", to="UTF-8")
)
corp <- corp %>%
mutate(txt.merged.utf8=ifelse(is.na(txt.merged.utf8), iconv(txt.merged, from = "UTF-8" , to ="WINDOWS-1252"), txt.merged.utf8),
search.term=search.term,
cat=ini)
# save
save(corp, file=paste0('corpus-', tolower(gsub('\\s+', '', search.term)), '-DE.RDS'))
}
}
corp
library(XML)
library(RCurl)
library(pbapply)
library(dplyr)
library(stringi)
library(pbmcapply)
library(textcat)
setwd('~/share/socsc1/output/sbi/')
rm(list=ls())
system("locale -a", intern = TRUE)
Sys.setlocale("LC_ALL", "it_IT")
source('~/r-helpers/google-search/google-search-fx.R')
source('~/r-helpers/selenium-hacks/check-http-status-codes-fx.R')
source('~/r-helpers/selenium-hacks/extract-txt-fx.R')
source('~/r-helpers/text-analysis/text-cleaning-fx.R')
source('~/r-helpers/url-snowballing/url-snowballing-fx.R')
search.terms <- read.csv('~/share/socsc1/input/sbi/search-term-list-SBI-IT.csv',
stringsAsFactors = F,
header = T) %>%
as_tibble %>%
setNames(., c('key', 'cat', 'regex')) %>%
as_tibble %>%
mutate(regex=gsub('\\,\\s', '|', regex)) %>%
mutate_all(.funs=function(x) gsub('^$', NA, x)) %>%
na.omit
search.terms
splits <- split(search.terms, search.terms$cat)
language <- 'it'
buzzers <- readLines('~/share/socsc1/input/buzzers.txt') %>%
unlist %>%
gsub('\\,', '', .) %>%
gsub('\\(\\\\', '(', .) %>%
unique
# load list with newspaper name
load('~/share/socsc1/input/liste-ch-zeitungen-wiki.RDS')
# some names a quite problematic, since they are liekly to occur in a normal sentence too...
# let's filter them out
newspapers <- newspapers[!newspapers%in%c('.ch',
'Der Bund',
'Die Hauptstadt',
'Die Heimat',
'News',
'heute',
'Saiten',
'Zürich',
'Zollikerberg',
'Der Eidgenosse',
'Das Volk',
'Die Nation',
'Die Woche')]
# define buzzwords
buzzwords <- c(buzzers,
tolower(newspapers)) %>%
gsub('\\.', '\\\\.', .)
buzzwords
# check for correct implementation of punctation
grep('\\.', buzzwords, value=T)
# define additional hardfilter grepl regex
hard.filter <- c('\\((IT|de|it|pt|es|en|ru|ar)\\)',
'([0-9]{2}\\:[0-9]{2}[A-z]+)|([0-9]{2}\\:[0-9]{3}\\s)|([0-9]{2}\\:[0-9]{4}\\.[0-9])',
'([A-Z][a-z]+[A-Z][a-z]+)')
for(sub.set in splits){
search.terms <- sub.set
print(search.terms)
category <- unique(search.terms$cat)
regex <- tolower(unique(search.terms$regex))
for(keys in search.terms$key){
print(paste(category, '-', keys, ': generate URLs' ))
# enter search term
ini <- category
search.term <- keys
## specify the URL for searches:
# quotes:: quoted search term (T/F)
# n.pages:: number pages that should be returned; if n.pages > 1 -> additional urls are gerated for each page
search.url <- get_search_url(search.term=search.term, language = language, quotes=F, n.pages=10)
## get hits back
# raw:: if you want the raw url (T/F)
# drop.recursive:: if you want to drop results ITom picture result suggestions etc. (T/F)
hits <- pblapply(search.url, function(x) get_google_hits(x, raw=F, drop.recursives = T))
# filter out all pages with no results
hits <- hits[lengths(hits)>0]
if(length(hits)==0){
Sys.sleep(60*10)
## get hits back
# raw:: if you want the raw url (T/F)
# drop.recursive:: if you want to drop results ITom picture result suggestions etc. (T/F)
hits <- pbmclapply(search.url, function(x) get_google_hits(x, raw=F, drop.recursives = T), mc.cores=2)
# filter out all pages with no results
hits <- hits[lengths(hits)>0]
}
# save hits
save(hits, file=paste0('hits-full-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
print(paste(category, '-', keys, ': HTTP check #1'))
# check if they can be accessed or are dead links (is able to handle lists and vectors!)
hits.b <- pbmclapply(hits, check_status_code, mc.cores = 4) %>% do.call(rbind, .)
# save checks
save(hits.b, file=paste0('hits-checked-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
print(paste(category, '-', keys, ': snowballing'))
## use the snowballing algorithm
# NOT RUN: we use a keyword-restriction, only collecting links containing (no-billag|nobillag)
snow.balls <- pbmclapply(hits.b$url[hits.b$boolean==T], function(x){
cycler(start=x,
time.limit=60,
keywords=regex,
key.match = 'inner'
)
}, mc.cores=4)
# save
save(snow.balls, file=paste0('snowballing-res-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
print(paste(category, '-', keys, ': HTTP check #2'))
# check them again
snow.balls.b <- pbmclapply(snow.balls, check_status_code, mc.cores = 4) %>% do.call(rbind, .)
save(snow.balls.b, file=paste0('snowballing-res-checked-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
# load(paste0('snowballing-res-checked-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
print(paste(category, '-', keys, ': text extraction'))
## get texts
# add.queries:: add additional queries; default: //p & //title
# preproc.expr:: additional regex expressions for preprocessing
# merged:: if T: return collapsed text (\n-sep); if F :returns a list with character strings (contents for each html-element)
# sort out pdfs/pngs/jpegs!!!!
txt <- pbmclapply(snow.balls.b$url[snow.balls.b$boolean==T&!grepl('pdf|PDF|png|PNG|jpeg|jpg|JPEG|JPG', snow.balls.b$url)], function(x){
extract_txt(x,
merged = FALSE,
add.queries = c(h2 = '//h2', h3 = '//h3', li='//li'),
preproc.expr = '(^(\\s+)?$)|(\\\n(\\s+)?)|(\\s{2,})'
)
}, mc.cores=5)
# save texts
save(txt, file=paste0('txt-extrctd-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
# load(paste0('txt-extrctd-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
print(paste(category, '-', keys, ': document burning'))
## clean texts
# raw:: if T: only returns annotated texts (tibble); if F: actually computes transformations
# hard.filter:: if grepl returns true on any of these elements, the respective line (== HTML-element) will be removed
# buzzwords:: additional buzzwords (default can be accessed by running `default.buzzwords`)
#         |_ buzzwords are used to indicate possible sclicing points and to compute filter conditions
# min.words:: minimum number of words per single character string (used for filtering, default: 3)
# min.avg.characters:: minimum number of avg.characters per word for each single character string (used for filtering, default: 3)
# max.buzzwords:: max. number of buzzwords allowed per character string (default: 2)
# sclicing:: whether slicing is allowed or not [slicing==deleting last part of text according to user-specified conditions] (T/F)
# slicing.keywords:: character string with additional slicing keywords (default can be accessed by running `default.slicing.keywords`)
#         |_ [!!!] each slicing keywords have to be identical to a buzzword
#         |_ [!!!] HENCE: sclicing.keyword is a subset of buzzwords
#         |_ [!!!] it is recommended to use them very cautiously, since they lead to deletions of whole text parts
# scnd.step.slicing:: last share of the document that will be used to compute the sum buzzwords (default: 3; meaning: last third of the document)
# scnd.step.threshold:: max number of buzzwords allowed in scnd.step.slicing before proceeding to slice the data (default: 4)
# define buzzwords
buzzwords <- c('bild',
'video',
'kontakt',
'links',
'(\\&)',
'registrier',
'vielen dank',
'wir wünschen ihnen',
'nzz',
'blick',
'anmeld',
'passwort',
'alle rechte',
'medienwoche',
'([0-9]{5,})',
'(\\|)',
'©',
'app',
'bitte',
'kommentar',
'kompaktansicht',
'weiterlesen',
'jetzt spenden',
'hol dir',
'…',
'woz',
'20 minuten',
'live',
'resultate',
'youtube',
'das könnte sie interessieren',
'fonction',
'sauter',
'navigation',
'recherche',
'all rights reserved',
'external link',
'click here',
'newsletter',
'signing up',
'swissinfo.ch',
'copyright',
'cookies',
'watson',
'radio life',
'erf medien',
'\\(c\\)',
'channel',
tolower(newspapers)) %>%
gsub('\\.', '\\\\.', .)
# check for correct implementation of punctation
grep('\\.', buzzwords, value=T)
# define additional hardfilter grepl regex
hard.filter <- c('\\((IT|de|it|pt|es|en|ru|ar)\\)',
'([0-9]{2}\\:[0-9]{2}[A-z]+)|([0-9]{2}\\:[0-9]{3}\\s)|([0-9]{2}\\:[0-9]{4}\\.[0-9])',
'([A-Z][a-z]+[A-Z][a-z]+)')
# call cleaner
txt.clean <- pbmclapply(txt, function(x){
clean_text(x,
hard.filter = hard.filter,
slicing = T,
min.words = 6,
max.buzzwords = 3,
scnd.step.slicing = 3,
scnd.step.threshold = 20,
buzzwords = buzzwords,
recover.fs = 50)
}, mc.cores=5)
txt.clean <- setNames(txt.clean, sapply(txt, colnames))
# save
save(txt.clean, file=paste0('txt-cleaned-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
# load(paste0('txt-cleaned-', tolower(gsub('\\s+', '', search.term)),'-', ini, '-IT.RDS'))
# get rid of elments that were single line documents (<- NA), and those that don't contain any information anymore after cleaning
txt.clean <- txt.clean[!is.na(txt.clean)] %>%
.[unlist(sapply(., nrow))>1]
# save
save(txt.clean, file=paste0('txt-cleaned-noNAs-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
# load(paste0('txt-cleaned-noNAs-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
print(paste(category, '-', keys, ': merging'))
# add language variable, merge texts, and convert them to true UTF-8
corp <- tibble(url=names(txt.clean),
txt.burnt=txt.clean,
language=pbsapply(txt.clean, function(x){textcat(paste0(x$txt, collapse = ' '))}),
txt.merged=sapply(txt.clean, function(x){paste0(x$txt, collapse = '\n')}),
txt.merged.utf8=iconv(txt.merged, from = "UTF-8", to = "MAC") %>% iconv(., from = "MAC", to="UTF-8")
)
corp <- corp %>%
mutate(txt.merged.utf8=ifelse(is.na(txt.merged.utf8), iconv(txt.merged, from = "UTF-8" , to ="WINDOWS-1252"), txt.merged.utf8),
search.term=search.term,
cat=ini)
# save
save(corp, file=paste0('corpus-', tolower(gsub('\\s+', '', search.term)), '-IT.RDS'))
}
}
corp
library(XML)
library(RCurl)
library(pbapply)
library(dplyr)
library(stringi)
library(pbmcapply)
library(textcat)
setwd('~/share/socsc1/output/sbi/')
rm(list=ls())
system("locale -a", intern = TRUE)
Sys.setlocale("LC_ALL", "fr_FR")
source('~/r-helpers/google-search/google-search-fx.R')
source('~/r-helpers/selenium-hacks/check-http-status-codes-fx.R')
source('~/r-helpers/selenium-hacks/extract-txt-fx.R')
source('~/r-helpers/text-analysis/text-cleaning-fx.R')
source('~/r-helpers/url-snowballing/url-snowballing-fx.R')
search.terms <- read.csv('~/share/socsc1/input/sbi/search-term-list-SBI-FR.csv',
stringsAsFactors = F,
header = T) %>%
as_tibble %>%
setNames(., c('key', 'cat', 'regex')) %>%
as_tibble %>%
mutate(regex=gsub('\\,\\s', '|', regex)) %>%
mutate_all(.funs=function(x) gsub('^$', NA, x)) %>%
na.omit
search.terms
splits <- split(search.terms, search.terms$cat)
language <- 'fr'
buzzers <- readLines('~/share/socsc1/input/buzzers.txt') %>%
unlist %>%
gsub('\\,', '', .) %>%
gsub('\\(\\\\', '(', .) %>%
unique
# load list with newspaper name
load('~/share/socsc1/input/liste-ch-zeitungen-wiki.RDS')
# some names a quite problematic, since they are liekly to occur in a normal sentence too...
# let's filter them out
newspapers <- newspapers[!newspapers%in%c('.ch',
'Der Bund',
'Die Hauptstadt',
'Die Heimat',
'News',
'heute',
'Saiten',
'Zürich',
'Zollikerberg',
'Der Eidgenosse',
'Das Volk',
'Die Nation',
'Die Woche')]
# define buzzwords
buzzwords <- c(buzzers,
tolower(newspapers)) %>%
gsub('\\.', '\\\\.', .)
buzzwords
# check for correct implementation of punctation
grep('\\.', buzzwords, value=T)
# define additional hardfilter grepl regex
hard.filter <- c('\\((FR|de|it|pt|es|en|ru|ar)\\)',
'([0-9]{2}\\:[0-9]{2}[A-z]+)|([0-9]{2}\\:[0-9]{3}\\s)|([0-9]{2}\\:[0-9]{4}\\.[0-9])',
'([A-Z][a-z]+[A-Z][a-z]+)')
for(sub.set in splits){
search.terms <- sub.set
print(search.terms)
category <- unique(search.terms$cat)
regex <- tolower(unique(search.terms$regex))
for(keys in search.terms$key){
print(paste(category, '-', keys, ': generate URLs' ))
# enter search term
ini <- category
search.term <- keys
## specify the URL for searches:
# quotes:: quoted search term (T/F)
# n.pages:: number pages that should be returned; if n.pages > 1 -> additional urls are gerated for each page
search.url <- get_search_url(search.term=search.term, language = language, quotes=F, n.pages=10)
## get hits back
# raw:: if you want the raw url (T/F)
# drop.recursive:: if you want to drop results FRom picture result suggestions etc. (T/F)
hits <- pblapply(search.url, function(x) get_google_hits(x, raw=F, drop.recursives = T))
# filter out all pages with no results
hits <- hits[lengths(hits)>0]
if(length(hits)==0){
Sys.sleep(60*10)
## get hits back
# raw:: if you want the raw url (T/F)
# drop.recursive:: if you want to drop results FRom picture result suggestions etc. (T/F)
hits <- pbmclapply(search.url, function(x) get_google_hits(x, raw=F, drop.recursives = T), mc.cores=2)
# filter out all pages with no results
hits <- hits[lengths(hits)>0]
}
# save hits
save(hits, file=paste0('hits-full-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
print(paste(category, '-', keys, ': HTTP check #1'))
# check if they can be accessed or are dead links (is able to handle lists and vectors!)
hits.b <- pbmclapply(hits, check_status_code, mc.cores = 4) %>% do.call(rbind, .)
# save checks
save(hits.b, file=paste0('hits-checked-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
print(paste(category, '-', keys, ': snowballing'))
## use the snowballing algorithm
# NOT RUN: we use a keyword-restriction, only collecting links containing (no-billag|nobillag)
snow.balls <- pbmclapply(hits.b$url[hits.b$boolean==T], function(x){
cycler(start=x,
time.limit=60,
keywords=regex,
key.match = 'inner'
)
}, mc.cores=4)
# save
save(snow.balls, file=paste0('snowballing-res-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
print(paste(category, '-', keys, ': HTTP check #2'))
# check them again
snow.balls.b <- pbmclapply(snow.balls, check_status_code, mc.cores = 4) %>% do.call(rbind, .)
save(snow.balls.b, file=paste0('snowballing-res-checked-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
# load(paste0('snowballing-res-checked-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
print(paste(category, '-', keys, ': text extraction'))
## get texts
# add.queries:: add additional queries; default: //p & //title
# preproc.expr:: additional regex expressions for preprocessing
# merged:: if T: return collapsed text (\n-sep); if F :returns a list with character strings (contents for each html-element)
# sort out pdfs/pngs/jpegs!!!!
txt <- pbmclapply(snow.balls.b$url[snow.balls.b$boolean==T&!grepl('pdf|PDF|png|PNG|jpeg|jpg|JPEG|JPG', snow.balls.b$url)], function(x){
extract_txt(x,
merged = FALSE,
add.queries = c(h2 = '//h2', h3 = '//h3', li='//li'),
preproc.expr = '(^(\\s+)?$)|(\\\n(\\s+)?)|(\\s{2,})'
)
}, mc.cores=5)
# save texts
save(txt, file=paste0('txt-extrctd-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
# load(paste0('txt-extrctd-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
print(paste(category, '-', keys, ': document burning'))
## clean texts
# raw:: if T: only returns annotated texts (tibble); if F: actually computes transformations
# hard.filter:: if grepl returns true on any of these elements, the respective line (== HTML-element) will be removed
# buzzwords:: additional buzzwords (default can be accessed by running `default.buzzwords`)
#         |_ buzzwords are used to indicate possible sclicing points and to compute filter conditions
# min.words:: minimum number of words per single character string (used for filtering, default: 3)
# min.avg.characters:: minimum number of avg.characters per word for each single character string (used for filtering, default: 3)
# max.buzzwords:: max. number of buzzwords allowed per character string (default: 2)
# sclicing:: whether slicing is allowed or not [slicing==deleting last part of text according to user-specified conditions] (T/F)
# slicing.keywords:: character string with additional slicing keywords (default can be accessed by running `default.slicing.keywords`)
#         |_ [!!!] each slicing keywords have to be identical to a buzzword
#         |_ [!!!] HENCE: sclicing.keyword is a subset of buzzwords
#         |_ [!!!] it is recommended to use them very cautiously, since they lead to deletions of whole text parts
# scnd.step.slicing:: last share of the document that will be used to compute the sum buzzwords (default: 3; meaning: last third of the document)
# scnd.step.threshold:: max number of buzzwords allowed in scnd.step.slicing before proceeding to slice the data (default: 4)
# define buzzwords
buzzwords <- c('bild',
'video',
'kontakt',
'links',
'(\\&)',
'registrier',
'vielen dank',
'wir wünschen ihnen',
'nzz',
'blick',
'anmeld',
'passwort',
'alle rechte',
'medienwoche',
'([0-9]{5,})',
'(\\|)',
'©',
'app',
'bitte',
'kommentar',
'kompaktansicht',
'weiterlesen',
'jetzt spenden',
'hol dir',
'…',
'woz',
'20 minuten',
'live',
'resultate',
'youtube',
'das könnte sie interessieren',
'fonction',
'sauter',
'navigation',
'recherche',
'all rights reserved',
'external link',
'click here',
'newsletter',
'signing up',
'swissinfo.ch',
'copyright',
'cookies',
'watson',
'radio life',
'erf medien',
'\\(c\\)',
'channel',
tolower(newspapers)) %>%
gsub('\\.', '\\\\.', .)
# check for correct implementation of punctation
grep('\\.', buzzwords, value=T)
# define additional hardfilter grepl regex
hard.filter <- c('\\((FR|de|it|pt|es|en|ru|ar)\\)',
'([0-9]{2}\\:[0-9]{2}[A-z]+)|([0-9]{2}\\:[0-9]{3}\\s)|([0-9]{2}\\:[0-9]{4}\\.[0-9])',
'([A-Z][a-z]+[A-Z][a-z]+)')
# call cleaner
txt.clean <- pbmclapply(txt, function(x){
clean_text(x,
hard.filter = hard.filter,
slicing = T,
min.words = 6,
max.buzzwords = 3,
scnd.step.slicing = 3,
scnd.step.threshold = 20,
buzzwords = buzzwords,
recover.fs = 50)
}, mc.cores=5)
txt.clean <- setNames(txt.clean, sapply(txt, colnames))
# save
save(txt.clean, file=paste0('txt-cleaned-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
# load(paste0('txt-cleaned-', tolower(gsub('\\s+', '', search.term)),'-', ini, '-FR.RDS'))
# get rid of elments that were single line documents (<- NA), and those that don't contain any information anymore after cleaning
txt.clean <- txt.clean[!is.na(txt.clean)] %>%
.[unlist(sapply(., nrow))>1]
# save
save(txt.clean, file=paste0('txt-cleaned-noNAs-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
# load(paste0('txt-cleaned-noNAs-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
print(paste(category, '-', keys, ': merging'))
# add language variable, merge texts, and convert them to true UTF-8
corp <- tibble(url=names(txt.clean),
txt.burnt=txt.clean,
language=pbsapply(txt.clean, function(x){textcat(paste0(x$txt, collapse = ' '))}),
txt.merged=sapply(txt.clean, function(x){paste0(x$txt, collapse = '\n')}),
txt.merged.utf8=iconv(txt.merged, from = "UTF-8", to = "MAC") %>% iconv(., from = "MAC", to="UTF-8")
)
corp <- corp %>%
mutate(txt.merged.utf8=ifelse(is.na(txt.merged.utf8), iconv(txt.merged, from = "UTF-8" , to ="WINDOWS-1252"), txt.merged.utf8),
search.term=search.term,
cat=ini)
# save
save(corp, file=paste0('corpus-', tolower(gsub('\\s+', '', search.term)), '-FR.RDS'))
}
}
corp
library(dplyr)
library(stringi)
rm(list = ls())
setwd('~/share/socsc1/output/')
setwd('~/share/socsc1/output/sbi/')
corpus <- lapply(grep('corpus', list.files(), value=T), function(x){
load(x)
corp
}) %>%
do.call(rbind, .) %>%
mutate(txt.merged.utf8=ifelse(is.na(txt.merged.utf8), txt.merged, txt.merged.utf8)) %>%
filter(!duplicated(url))
corpus
save(corpus, file='CORPUS-sbi.RDS')
system('cp -a ~/Dropbox/share/socsc1/output/ ~/share/socsc1/output-directLink/')
